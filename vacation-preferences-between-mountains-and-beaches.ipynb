{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9706091,"sourceType":"datasetVersion","datasetId":5936188}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bedirhankaraahmetli/vacation-preferences-between-mountains-and-beaches?scriptVersionId=217849087\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **0 - Introduction**","metadata":{}},{"cell_type":"markdown","source":"## **Project Overview**","metadata":{}},{"cell_type":"markdown","source":"In recent years, understanding consumer preferences has become a cornerstone of personalized marketing and service delivery. One area where such understanding is particularly valuable is in the tourism and travel industry. Peopleâ€™s vacation choices, such as preferring mountains or beaches, are influenced by various demographic and lifestyle factors, including age, income, travel habits, and environmental concerns. Analyzing these factors and predicting preferences can help businesses optimize their offerings and provide a more tailored customer experience.\n\nThis project focuses on predicting vacation preferences between mountains and beaches using demographic data. By leveraging a dataset of over 52,000 individuals, we aim to explore patterns and relationships in the data, apply data mining techniques to extract insights, and develop machine learning models to forecast preferences. The project will integrate various stages, including data exploration, preprocessing, model building, evaluation, deployment, and results interpretation.","metadata":{}},{"cell_type":"markdown","source":"## **Objective**","metadata":{}},{"cell_type":"markdown","source":"The primary objective of this project is to build predictive models that classify individuals' vacation preferences as either \"mountains\" or \"beaches\" based on their demographic and lifestyle attributes. Additionally, the project seeks to achieve the following goals:\n\n- Data Analysis: Explore and understand the dataset to uncover trends and relationships that influence vacation preferences.\n- Model Development: Experiment with various machine learning algorithms to identify the most effective approach for prediction.\n- Feature Insights: Determine the importance of specific features (e.g., income, proximity to vacation spots) in predicting preferences.\n- Practical Application: Deploy a predictive model that can be used by travel-related businesses to personalize marketing campaigns and services.\n- Educational Value: Gain hands-on experience in applying data mining and machine learning techniques in a real-world scenario.","metadata":{}},{"cell_type":"markdown","source":"## **Dataset Overview**","metadata":{}},{"cell_type":"markdown","source":"The dataset used for this project comprises 52,444 entries and 13 features, offering a comprehensive view of the factors that may influence vacation preferences. Each entry represents an individual, with attributes categorized as numerical, categorical, or binary. Below is a summary of the dataset features:\n\n- Age: Age of the individual (numerical).\n- Gender: Gender identity (categorical: male, female, non-binary).\n- Income: Annual income of the individual (numerical).\n- Education Level: Highest level of education attained (categorical: high school, bachelor, master, doctorate).\n- Travel Frequency: Number of vacations taken per year (numerical).\n- Preferred Activities: Activities preferred during vacations (categorical: hiking, swimming, skiing, sunbathing).\n- Vacation Budget: Budget allocated for vacations (numerical).\n- Location: Type of residence (categorical: urban, suburban, rural).\n- Proximity to Mountains: Distance from the nearest mountains (numerical, in miles).\n- Proximity to Beaches: Distance from the nearest beaches (numerical, in miles).\n- Favorite Season: Preferred season for vacations (categorical: summer, winter, spring, fall).\n- Pets: Ownership of pets (binary: 0 = No, 1 = Yes).\n- Environmental Concerns: Environmental awareness or concerns (binary: 0 = No, 1 = Yes).\n\nThis diverse dataset allows for the application of multiple data mining and machine learning techniques, making it ideal for predictive modeling. Through careful analysis and modeling, this project aims to provide meaningful insights into vacation preferences and build an effective predictive tool.","metadata":{}},{"cell_type":"markdown","source":"# **1 - Data Understanding and Preprocessing**","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.1 Loading the Dataset","metadata":{}},{"cell_type":"code","source":"filepath = \"/kaggle/input/mountains-vs-beaches-preference/mountains_vs_beaches_preferences.csv\"\ndf = pd.read_csv(filepath, comment='#')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.2 Inspecting dataset","metadata":{}},{"cell_type":"code","source":"def inspect_dataset(dataframe):\n    print(\"\\nDataset Info:\")\n    dataframe.info()\n    print(\"\\nSummary Statistics:\")\n    print(dataframe.describe())\n    print(\"\\nChecking for missing values:\")\n    print(dataframe.isnull().sum())\n\ninspect_dataset(df)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting features\nnumerical_features = ['Age', 'Income', 'Travel_Frequency', 'Vacation_Budget', 'Proximity_to_Mountains', 'Proximity_to_Beaches']\ncategorical_features = ['Gender', 'Education_Level', 'Preferred_Activities', 'Location', 'Favorite_Season']\nbinary_features = ['Pets', 'Environmental_Concerns']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nChecking for missing values:\")\nprint(df.isnull().sum())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can clearly say there isn't any missing values. Therefore, no further steps are necessary about handling missing values","metadata":{}},{"cell_type":"markdown","source":"## 1.3 Detecting and Handling Outliers","metadata":{}},{"cell_type":"code","source":"def plot_outliers(dataframe, numerical_columns):\n    plt.figure(figsize=(16, 8))\n    for i, col in enumerate(numerical_columns, start=1):\n        plt.subplot(2, 3, i)\n        sns.boxplot(y=dataframe[col])\n        plt.title(f\"Boxplot of {col}\")\n        plt.ylabel('')\n        plt.xlabel(col)\n    plt.tight_layout()\n    plt.show()\n\n# Visualize outliers for numerical features\nplot_outliers(df, numerical_features)\n\n# Numerical validation of outliers using IQR\ndef detect_outliers(dataframe, column):\n    Q1 = dataframe[column].quantile(0.25)\n    Q3 = dataframe[column].quantile(0.75)\n    IQR = Q3 - Q1\n    outliers = dataframe[(dataframe[column] < Q1 - 1.5 * IQR) | (dataframe[column] > Q3 + 1.5 * IQR)]\n    return outliers\n\n# Check for outliers in numerical features\nfor col in numerical_features:\n    outliers = detect_outliers(df, col)\n    print(f\"{col}: {len(outliers)} outliers detected.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see from the boxplots above,\nthere aren't any outliers detected for numerical fetaures","metadata":{}},{"cell_type":"markdown","source":"## 1.4 Encoding Categorical Features","metadata":{}},{"cell_type":"code","source":"df = pd.get_dummies(df, columns=categorical_features, drop_first=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('\\nDataset After Encoding:')\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.5 Normalizing Numerical Features","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf[numerical_features] = scaler.fit_transform(df[numerical_features])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('\\nDataset After Scaling:')\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Boxplots for numerical features after encoding and scaling\nplt.figure(figsize=(16, 8))\nfor i, col in enumerate(numerical_features, start=1):\n    plt.subplot(2, 3, i)\n    sns.boxplot(y=df[col])\n    plt.title(f\"Boxplot of {col} After Scaling\")\n    plt.ylabel('')\n    plt.xlabel(col)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Histograms for numerical features after encoding and scaling\nplt.figure(figsize=(16, 8))\nfor i, col in enumerate(numerical_features, start=1):\n    plt.subplot(2, 3, i)\n    sns.histplot(df[col], kde=True, bins=20)\n    plt.title(f\"Histogram of {col} After Scaling\")\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2 - Data Preparation","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nfrom sklearn.model_selection import train_test_split, StratifiedKFold","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since there isn't much feature, we decided to do feature selection out of the k-fold cross validation loop once, then proceed to the model training steps","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Feature Selection","metadata":{}},{"cell_type":"markdown","source":"### 2.1.1 Correlation Matrix","metadata":{}},{"cell_type":"code","source":"# Correlation Analysis\ndef plot_correlation_matrix(dataframe):\n    correlation_matrix = dataframe.corr()\n    plt.figure(figsize=(16, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Matrix')\n    plt.show()\n    return correlation_matrix\n\ncorrelation_matrix = plot_correlation_matrix(df)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a list to store features with correlation greater than the threshold\ncorrelated_features = set()\nthreshold = 0.8","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identifying highly correlated features\ncorrelated_features = set()\nthreshold = 0.8\n\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > threshold:\n            feature_1 = correlation_matrix.columns[i]\n            feature_2 = correlation_matrix.columns[j]\n            correlated_features.add((feature_1, feature_2))\n\n# Print pairs of highly correlated features\nprint(f\"Highly Correlated Feature Pairs (threshold > {threshold}): {correlated_features}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Based on the correlation matrix there isn't features highly correlated (> 0.8) with each other","metadata":{}},{"cell_type":"markdown","source":"### 2.1.2 Recursive Feature Elimination (RFE)","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define features (X) and target (y)\nX = df.drop(columns='Preference')\ny = df['Preference']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# RFE using RandomForestClassifier\nrfe_model = RandomForestClassifier(random_state=1905)\nrfe = RFE(estimator=rfe_model, n_features_to_select=10)\nX_rfe = rfe.fit_transform(X, y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting selected feature names\nselected_features = X.columns[rfe.support_]\nprint(f\"Selected Features: {selected_features}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter the dataset with selected features\nX = df[selected_features]\ny = df['Preference']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the filtered dataset\nprint(\"\\nFiltered Dataset with Selected Features:\")\nprint(X.head())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3 - Model Training and Evaluation**","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import (\n    accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n)\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport itertools","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppressing unnecessary warnings to keep the output clean\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting the dataset into train and test sets\nprint(\"Splitting the dataset into training and test sets...\")\nX_train, X_test, y_train, y_test = train_test_split(X_rfe, y, test_size=0.1, random_state=1905, stratify=y)\nprint(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initializing 9-fold cross-validation\nprint(\"\\nInitializing 9-fold cross-validation...\")\nn_splits = 9\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1905)\nfolds = list(skf.split(X_train, y_train))\nprint(f\"Number of folds: {n_splits}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Defining models and their hyperparameter grids\nprint(\"\\nDefining models and their hyperparameter grids...\")\nmodels = {\n    \"Logistic Regression\": {\n        \"model\": LogisticRegression(max_iter=1000, random_state=1905, solver='liblinear'),\n        \"param_grid\": {\n            \"penalty\": [\"l1\", \"l2\"],\n            \"C\": [0.01, 0.1, 1, 10]\n        }\n    },\n    \"Decision Tree\": {\n        \"model\": DecisionTreeClassifier(random_state=1905),\n        \"param_grid\": {\n            \"max_depth\": [None, 5, 7, 10],\n            \"min_samples_split\": [2, 5, 10]\n        }\n    },\n    \"Random Forest\": {\n        \"model\": RandomForestClassifier(random_state=1905),\n        \"param_grid\": {\n            \"n_estimators\": [50, 75, 100],\n            \"max_depth\": [None, 5, 10],\n            \"min_samples_split\": [2, 5, 10]\n        }\n    },\n    \"Support Vector Machine\": {\n        \"model\": SVC(random_state=1905, probability=True), # `probability=True` enables probability predictions\n        \"param_grid\": {\n            \"C\": [0.1, 1, 10],\n            \"kernel\": [\"linear\", \"rbf\"]\n        }\n    },\n    \"Naive Bayes\": {\n        \"model\": GaussianNB(),\n        \"param_grid\": {}  # No hyperparameters to tune\n    },\n    \"K-Nearest Neighbors\": {\n        \"model\": KNeighborsClassifier(),\n        \"param_grid\": {\n            \"n_neighbors\": [3, 5, 7, 9],\n            \"weights\": [\"uniform\", \"distance\"]\n        }\n    }\n}\nprint(\"Models and hyperparameter grids defined successfully.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grid search function\ndef custom_grid_search(model, X_train, y_train, X_val, y_val, param_grid):\n    \"\"\"\n    Perform grid search for hyperparameter tuning.\n    If the parameter grid is empty, train the model directly.\n    \"\"\"\n    print(\"\\tPerforming grid search...\")\n    best_params = None  # To store the best parameter combination\n    best_score = 0  # To store the highest validation accuracy\n    results = []  # To store all parameter combinations and their corresponding scores\n\n    # If no parameters to tune, train the model directly\n    if not param_grid:\n        print(\"\\tNo parameters to tune for this model. Training directly...\")\n        model.fit(X_train, y_train)  # Train the model\n        val_accuracy = model.score(X_val, y_val)  # Evaluate the model\n        results.append(({}, val_accuracy))\n        return {}, val_accuracy, results\n\n    # Generate all combinations of parameters\n    keys, values = zip(*param_grid.items())\n    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n\n    # Evaluate each parameter combination\n    for params in param_combinations:\n        try:\n            # Set parameters and fit the model\n            model.set_params(**params)\n            model.fit(X_train, y_train)\n\n            # Evaluate on validation set\n            val_accuracy = model.score(X_val, y_val)\n            results.append((params, val_accuracy))\n\n            # Update best parameters if current accuracy is higher\n            if val_accuracy > best_score:\n                best_score = val_accuracy\n                best_params = params\n        except Exception as e:\n            print(f\"\\tSkipping parameters {params} due to error: {e}\")\n\n    print(f\"\\tGrid search completed. Best parameters: {best_params}, Best score: {best_score:.4f}\")\n    return best_params, best_score, results","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to plot grid search results\ndef plot_grid_search_results(results, fold_idx):\n    \"\"\"\n    Plot grid search results for a specific fold using a horizontal bar chart.\n    \"\"\"\n    print(f\"\\tPlotting grid search results for Fold {fold_idx + 1}...\")\n    results_df = pd.DataFrame([\n        {\"Params\": str(params), \"Accuracy\": accuracy}\n        for params, accuracy in results\n    ])\n    results_df.sort_values(by=\"Accuracy\", ascending=False, inplace=True)\n\n    plt.figure(figsize=(12, 8))\n    sns.barplot(\n        y=results_df[\"Params\"],\n        x=results_df[\"Accuracy\"],\n        palette=\"coolwarm\",\n        orient=\"h\"\n    )\n\n    for i, acc in enumerate(results_df[\"Accuracy\"]):\n        plt.text(acc + 0.002, i, f\"{acc:.4f}\", va=\"center\", ha=\"left\", fontsize=10)\n\n    plt.xlim(0.8, 1.0)  # Adjust the x-axis range for better visualization\n    plt.xlabel(\"Accuracy\")\n    plt.ylabel(\"Parameter Combination\")\n    plt.title(f\"Grid Search Results for Fold {fold_idx + 1}\")\n    plt.tight_layout()\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Metrics collection and evaluation\nprint(\"\\nStarting model evaluation and metrics collection...\")\nall_results = []  # To store evaluation results for all models","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loop through each model\nfor model_name, model_info in models.items():\n    print(f\"\\nStarting Cross-Validation for {model_name}...\\n\")\n    model = model_info[\"model\"]  # Extract model\n    param_grid = model_info[\"param_grid\"]  # Extract parameter grid\n    \n    # Cross-validation for each fold\n    best_params_overall = None  # Best parameters across all folds\n    best_score_overall = 0  # Best score across all folds\n    fold_accuracies = []  # Store fold accuracies\n    training_errors = []  # Store training errors\n    validation_errors = []  # Store validation errors\n\n    for fold_idx, (train_idx, val_idx) in enumerate(folds):\n        print(f\"\\n\\tStarting Fold {fold_idx + 1}...\")\n        \n        # Split training and validation data for the current fold\n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n        # Perform grid search\n        best_params, best_score, results = custom_grid_search(model, X_fold_train, y_fold_train, X_fold_val, y_fold_val, param_grid)\n        fold_accuracies.append(best_score)  # Append fold score\n\n        # Track the best overall parameters\n        if best_score > best_score_overall:\n            best_score_overall = best_score\n            best_params_overall = best_params\n\n        # Compute training and validation errors\n        model.set_params(**best_params)\n        model.fit(X_fold_train, y_fold_train)  # Train the model\n        training_error = 1 - accuracy_score(y_fold_train, model.predict(X_fold_train))  # Training error\n        validation_predictions = model.predict(X_fold_val)  # Validation predictions\n        validation_error = 1 - accuracy_score(y_fold_val, validation_predictions)  # Validation error\n        training_errors.append(training_error)\n        validation_errors.append(validation_error)\n\n        # Plot grid search results (skip for Naive Bayes)\n        if model_name != \"Naive Bayes\":\n            plot_grid_search_results(results, fold_idx)\n\n        print(f\"\\tEnding Fold {fold_idx + 1}.\")\n\n    # Retrain the model with the best parameters on the full training set\n    print(f\"\\nRetraining the {model_name} on the full training set using the best parameters...\")\n    model.set_params(**best_params_overall)\n    model.fit(X_train, y_train)\n\n    # Evaluate on the test set\n    print(f\"\\nEvaluating {model_name} on the test set...\")\n    y_test_pred = model.predict(X_test)\n    y_test_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n\n    # Compute test set metrics\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n    test_report = classification_report(y_test, y_test_pred, output_dict=True)\n\n    # Save test set metrics for model comparison\n    all_results.append({\n        \"Model\": model_name,\n        \"Best Params Overall\": best_params_overall,\n        \"Average Fold Accuracy\": np.mean(fold_accuracies),\n        \"Test Set Metrics\": {\n            \"Accuracy\": test_accuracy,\n            \"Precision\": test_report[\"weighted avg\"][\"precision\"],\n            \"Recall\": test_report[\"weighted avg\"][\"recall\"],\n            \"F1-Score\": test_report[\"weighted avg\"][\"f1-score\"]\n        }\n    })\n\n    print(f\"\\nBest Parameters for {model_name}: {best_params_overall}\")\n    print(f\"Average Accuracy across all folds for {model_name}: {np.mean(fold_accuracies):.4f}\")\n    print(f\"Test Set Accuracy: {test_accuracy:.4f}\\n\")\n\n    # Plot learning curves\n    print(f\"\\nPlotting learning curves for {model_name}...\")\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(training_errors) + 1), training_errors, label=\"Training Error\", marker=\"o\")\n    plt.plot(range(1, len(validation_errors) + 1), validation_errors, label=\"Validation Error\", marker=\"o\")\n    plt.title(f\"Learning Curves for {model_name}\")\n    plt.xlabel(\"Fold\")\n    plt.ylabel(\"Error\")\n    plt.legend()\n    plt.grid(alpha=0.5)\n    plt.tight_layout()\n    plt.show()\n\n    # Plot ROC-AUC curve (if applicable)\n    if y_test_prob is not None:\n        print(f\"Plotting ROC-AUC curve for {model_name}...\")\n        fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n        roc_auc = auc(fpr, tpr)\n        plt.figure(figsize=(8, 6))\n        plt.plot(fpr, tpr, label=f\"ROC-AUC (AUC = {roc_auc:.4f})\")\n        plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n        plt.title(f\"ROC-AUC Curve for {model_name}\")\n        plt.xlabel(\"False Positive Rate\")\n        plt.ylabel(\"True Positive Rate\")\n        plt.legend()\n        plt.grid(alpha=0.5)\n        plt.tight_layout()\n        plt.show()\n\n    # Plot confusion matrix\n    print(f\"Plotting confusion matrix for {model_name} on the test set...\")\n    conf_matrix = confusion_matrix(y_test, y_test_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=model.classes_)\n    disp.plot(cmap=\"Blues\")\n    plt.title(f\"Confusion Matrix for {model_name} on Test Set\")\n    plt.tight_layout()\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare models\nprint(\"\\nCreating model comparison table...\")\ncomparison_df = pd.DataFrame([\n    {\n        \"Model\": result[\"Model\"],\n        \"Test Accuracy\": result[\"Test Set Metrics\"][\"Accuracy\"],\n        \"Test Precision\": result[\"Test Set Metrics\"][\"Precision\"],\n        \"Test Recall\": result[\"Test Set Metrics\"][\"Recall\"],\n        \"Test F1-Score\": result[\"Test Set Metrics\"][\"F1-Score\"]\n    }\n    for result in all_results\n])\nprint(comparison_df)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choose the best model\nbest_model_row = comparison_df.sort_values(by=\"Test Accuracy\", ascending=False).iloc[0]\nbest_model_name = best_model_row[\"Model\"]\nprint(f\"\\nBest Model: {best_model_name} based on Test Accuracy.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrain the best model on the entire dataset\nprint(f\"\\nTraining the best model ({best_model_name}) on the entire dataset (including test set)...\")\nfull_X = np.vstack((X_train, X_test))\nfull_y = pd.concat([y_train, y_test])\nbest_model = models[best_model_name][\"model\"]\nbest_model.set_params(**all_results[comparison_df[comparison_df[\"Model\"] == best_model_name].index[0]][\"Best Params Overall\"])\nbest_model.fit(full_X, full_y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate and predict 10 random instances\nprint(\"\\nGenerating 10 random instances and predicting their outcomes...\")\nrandom_instances = np.random.rand(10, full_X.shape[1])\nrandom_predictions = best_model.predict(random_instances)\nprint(\"\\nRandom Instances Predictions:\")\nfor i, prediction in enumerate(random_predictions, 1):\n    print(f\"Instance {i}: Predicted Class = {prediction}\")","metadata":{},"outputs":[],"execution_count":null}]}